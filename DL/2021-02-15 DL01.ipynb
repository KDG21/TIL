{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboardX\n",
    "# !pip install jupyter-tensorboard\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install h5py\n",
    "# !python -m pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[12]], shape=(1, 1), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 1  4]\n",
      " [ 9 16]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[2 4]\n",
      " [6 8]], shape=(2, 2), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 임포트 하자\n",
    "import tensorflow as tf\n",
    "\n",
    "# 행과 열을 만들자\n",
    "matrix1 = tf.constant([[3,3]])\n",
    "matrix2 = tf.constant([[2],[2]])\n",
    "\n",
    "res = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "print(res, '\\n')\n",
    "\n",
    "tf.constant([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "x = tf.constant(([1,2,3,4]), shape=(2,2))\n",
    "res02 = tf.math.multiply(x, x)\n",
    "print(res02, '\\n')\n",
    "\n",
    "hap = tf.add(x, x)\n",
    "print(hap, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7.]], shape=(1, 1), dtype=float32) \n",
      "\n",
      "tf.Tensor([[21.]], shape=(1, 1), dtype=float32) \n",
      "\n",
      "tf.Tensor([[21.]], shape=(1, 1), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 행렬 연산을 구현해보자 \n",
    "input01 = tf.constant([[3.0]])\n",
    "input02 = tf.constant([[2.0]])\n",
    "input03 = tf.constant([[5.0]])\n",
    "\n",
    "hap = tf.add(input02, input03)\n",
    "res = tf.multiply(input01, hap)\n",
    "print(hap, '\\n')\n",
    "print(res, '\\n')\n",
    "\n",
    "res02 = tf.multiply(input01, tf.add(input02, input03))\n",
    "print(res02, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      "\n",
      "hello [[4]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 즉시 기본 실행 : 메소드가 호출되는 시점이 가장 먼저 실행\n",
    "print(tf.executing_eagerly(), '\\n')\n",
    "\n",
    "x = [[2]]\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "print('hello {}'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> \n",
      "\n",
      "(2, 2) \n",
      "\n",
      "<dtype: 'int32'> <class 'tensorflow.python.framework.dtypes.DType'> \n",
      "\n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "int32 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 4. 반환 작업 : 어레이 객체를 리턴받는 것\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "\n",
    "print(a, '\\n')\n",
    "print(type(a), '\\n')\n",
    "print(a.shape, '\\n')\n",
    "print(a.dtype, type(a.dtype), '\\n')\n",
    "print(a.numpy(), '\\n')\n",
    "print(a.numpy().dtype, type(a.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 5. 브로드캐스팅(Broadcasting) 지원\n",
    "b = tf.add(a, 1)\n",
    "print(b, '\\n')\n",
    "\n",
    "# 6. 연산자 오버로딩 지원\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 7. numpy 지원\n",
    "import numpy as np\n",
    "\n",
    "res = np.multiply(a,b)\n",
    "print(res, type(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xed\\x99\\x8d\\xea\\xb8\\xb8\\xeb\\x8f\\x99' \n",
      "\n",
      "190 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'\\xed\\x99\\x8d\\xea\\xb8\\xb8\\xeb\\x8f\\x99'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Variable\n",
    "name = tf.Variable('홍길동', tf.string)\n",
    "kor = tf.Variable(100, tf.int32)\n",
    "eng = tf.Variable(90, tf.int32)\n",
    "tot = kor + eng\n",
    "\n",
    "print(name.numpy(), '\\n')\n",
    "print(tot.numpy(), '\\n')\n",
    "\n",
    "name02 = tf.Variable(['홍길동'], tf.string)\n",
    "name02.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([3.000000e+00, 1.234500e+04, 2.987976e+00], dtype=float32)> <tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([1, 2, 3, 4, 5])> <tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([12.3-4.85j,  7.5-6.12j])> \n",
      "\n",
      "[3.000000e+00 1.234500e+04 2.987976e+00] \n",
      "\n",
      "[1 2 3 4 5] \n",
      "\n",
      "[12.3-4.85j  7.5-6.12j]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([3, 12345, 2.987976], tf.float32)\n",
    "b = tf.Variable([1,2,3,4,5], tf.float32)\n",
    "c = tf.Variable([12.3-4.85j, 7.5-6.12j], tf.complex64)\n",
    "\n",
    "print(a, b, c, '\\n')\n",
    "print(a.numpy(), '\\n')\n",
    "print(b.numpy(), '\\n')\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(10, 299, 299, 3), dtype=float32) \n",
      "\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> \n",
      "\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 9. tensor 차원을 리턴 받자  tf.zeros(), tf.rank()\n",
    "# 배치 * 높이 * 너비 * 색상\n",
    "\n",
    "my_img = tf.zeros([10, 299, 299, 3])\n",
    "res = tf.rank(my_img)\n",
    "\n",
    "print(my_img, '\\n')\n",
    "print(type(my_img), '\\n')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 10. 요소를 리턴\n",
    "my_v = tf.Variable([1,2,3,4])\n",
    "my_s = my_v[2]\n",
    "\n",
    "print(my_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 11. tf의 객체를 Variable로 연동해보자\n",
    "my_val = tf.Variable(tf.zeros(2,2,3))  # 변수\n",
    "print(my_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 11. tf의 객체를 Variable로 연동해보자\n",
    "my_val = tf.constant(tf.zeros(2,2,3))  # 상수\n",
    "print(my_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0> \n",
      "\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 12. 변수 자동 변환\n",
    "v = tf.Variable(0.0)\n",
    "v.numpy()\n",
    "\n",
    "w = v+1  # w는 v값 기준으로 계산되는 tf.Tensor, 변수가 수식으로 사용하게 되면 tf.Tensor로 변환\n",
    "w\n",
    "\n",
    "v = tf.Variable(0.0)\n",
    "v.assign_add(1)\n",
    "\n",
    "print(v, '\\n')\n",
    "print(v.read_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 = 스칼라 또는 벡터 또는 매트릭스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]] \n",
      "\n",
      "[[-1. -2.]\n",
      " [-3. -4.]\n",
      " [-5. -6.]] \n",
      "\n",
      "<bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)>> \n",
      "\n",
      "<bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.01617927, -0.02819046],\n",
      "       [-0.15087171, -0.0390345 ],\n",
      "       [-0.07301226,  0.03693333]], dtype=float32)>> \n",
      "\n",
      "<bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 2.7738037 ,  2.1032095 ],\n",
      "       [ 1.0916338 , -0.57768965],\n",
      "       [ 1.3006043 ,  2.74863   ]], dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 3*2행의 float32값을 이용해서 값을 구현해보자\n",
    "'''\n",
    "1  2\n",
    "3  4\n",
    "5  6\n",
    "'''\n",
    "A = tf.Variable([[1,2],[3,4],[5,6]], dtype = tf.float32)\n",
    "print(A.numpy(), '\\n')\n",
    "\n",
    "# 전체 값을 -로 변경해보자\n",
    "A.assign([[-1,-2], [-3,-4], [-5,-6]])\n",
    "print(A.numpy(), '\\n')\n",
    "\n",
    "# 3*2행의 값을 모두 0으로 초기화. float32\n",
    "A = tf.Variable(tf.zeros([3,2]), dtype=tf.float32)\n",
    "print(A.numpy, '\\n')\n",
    "\n",
    "# 3*2 행의 값 요소모두를 평균은 0, 표준편차 0.1의 정규난수로 초기화를 시켜보자\n",
    "# tf.random.normal()\n",
    "A = tf.Variable(tf.random.normal([3,2], mean=0.0, stddev=0.1, dtype=tf.dtypes.float32))\n",
    "print(A.numpy, '\\n')\n",
    "\n",
    "# 3*2 행의 값 요소모두를 -1 ~ 3 까지의 범위의 균일값을 난수로 초기화 시켜라\n",
    "A = tf.Variable(tf.random.uniform([3,2], minval=-1, maxval=3, dtype=tf.dtypes.float32))\n",
    "print(A.numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.6931472 1.0986123] \n",
      "\n",
      "[0.7310586  0.8807971  0.95257413] \n",
      "\n",
      "[0.7615942 0.9640276 0.9950547]\n"
     ]
    }
   ],
   "source": [
    "# 13. f(x) 표현하는 연산 시스템\n",
    "# log(x), exp(x), sin(x), cos(x), sigmoid(x), tanh(x) 등\n",
    "\n",
    "A = tf.constant([1,2,3], tf.float32)\n",
    "B = tf.math.log(A)\n",
    "print(B.numpy(), '\\n')\n",
    "\n",
    "B = tf.math.sigmoid(A)\n",
    "print(B.numpy(), '\\n')\n",
    "\n",
    "B = tf.math.tanh(A)\n",
    "print(B.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14. softmax : 로지스틱 함수를 다차원으로 만든것\n",
    "다항 로지스틱 회귀로 출력에 대한 확률분포 네트워크 출력을 하는 정규화\n",
    "신경망의 활성화 함수로 자주 사용한다.\n",
    "z = K로 실수값을 벡터로 받아서 입력 숫자에 지수에 비례하는 K개의 확률분포 정규화 시킨것\n",
    "\n",
    "tf.nn.softmax(logits, axis, name)\n",
    "softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "\n",
    "tf.nn.softmax(x)\n",
    "입력 인수 x : 텐서, 정수 상수 텐서, 변수 상수\n",
    "출력 값 : 1. x의 각 요소마다 exp()을 계산해서 A에 할당\n",
    "          2. A의 요소를 모두 더해서 임의 변수 a에 넣는다\n",
    "          3. A의 요소를 a로 나누어서 출력 텐서를 한다\n",
    "          4. 출력 텐서 각 요소의 값은 확률은 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472848 0.66524094] \n",
      "\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([1,2,3], tf.float32)\n",
    "B = tf.nn.softmax(A)\n",
    "print(B.numpy(), '\\n')   # 확률 분포\n",
    "\n",
    "B = tf.reduce_sum(A)\n",
    "print(B.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([20. 36.], shape=(2,), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'> \n",
      "\n",
      "[20. 36.]\n"
     ]
    }
   ],
   "source": [
    "# 15. 사용자 오퍼레이션(노드) 만드는 방법\n",
    "'''\n",
    "@tf.function\n",
    "def 함수명():\n",
    "    연산\n",
    "    return\n",
    "'''\n",
    "\n",
    "# (a+b)*c  - myOP(a,b,c)\n",
    "@tf.function\n",
    "def myOP(a,b,c):\n",
    "    t = tf.math.add(a,b)\n",
    "    return tf.math.multiply(t,c)\n",
    "\n",
    "A = tf.constant([1,2], tf.float32)\n",
    "B = tf.constant([3,4], tf.float32)\n",
    "C = tf.constant([5,6], tf.float32)\n",
    "D = myOP(A,B,C)\n",
    "print(D, type(D), '\\n')\n",
    "print(D.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 \n",
      "\n",
      "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=2>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=6>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=7>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=8>, <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=9>) \n",
      "\n",
      "<__main__.MyModuleOne object at 0x000001FD8A356C10> \n",
      "\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 16. 사용자 자료형 만들기\n",
    "# tf.Module를 상속받아 구현한다\n",
    "# tf.Module 인스턴스는 Variable을 포함해서 다른 모듈들을 탐색할 수 있다.\n",
    "# trainable_variables = 훈련가능한 변수를 리턴\n",
    "\n",
    "class MyModuleOne(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.VO = tf.Variable(1.0)\n",
    "        self.VS = [tf.Variable(x) for x in range(10)]\n",
    "        \n",
    "class MyOtherModule(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.m = MyModuleOne()\n",
    "        self.v = tf.Variable(10.0)\n",
    "        \n",
    "m = MyOtherModule()\n",
    "print(len(m.variables), '\\n')\n",
    "print(m.variables, '\\n')\n",
    "print(m.m, '\\n')  # 11은 m.m에서 다른값은 m.v에서\n",
    "print(m.v, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1fd8a53a700>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 17. 케라스\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Layer_0 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 101,121\n",
      "Trainable params: 101,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<class 'tensorflow.python.keras.layers.core.Dense'> \n",
      "\n",
      "Layer_0 \n",
      "\n",
      "{'Layer_0': 0, 'Layer_1': 1, 'Layer_2': 2} \n",
      "\n",
      "1 \n",
      "\n",
      "['Layer_0', 'Layer_1', 'Layer_2']\n"
     ]
    }
   ],
   "source": [
    "# 18. 순차모델을 만들어보자\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(100, name='Layer_0', input_shape=(1000,)),\n",
    "        tf.keras.layers.Dense(10, name='Layer_1'),\n",
    "        tf.keras.layers.Dense(1, name='Layer_2'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(type(model.layers[0]), '\\n')\n",
    "print(model.layers[0].name, '\\n')\n",
    "\n",
    "d = {k.name : i for i, k in enumerate(model.layers)}\n",
    "print(d, '\\n')\n",
    "print(d['Layer_1'], '\\n')\n",
    "\n",
    "layer_name = [k.name for k in model.layers]\n",
    "print(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "심층 신경망은 학습한 모든 변환을 수치 데이터 텐서에 적용하는 텐서 연산을 사용한다.\n",
    "  ex) 텐서 덧셈, 텐서 곱셈\n",
    "    \n",
    "tf.keras.layers.Dense(1, name='Layer_2')  -> 신경망을 만들었다. 층을 설정했다.\n",
    "\n",
    "데이터 -> 학습 구조 모델링 -> 학습 수행 -> 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[ 3  5  7  9 11] \n",
      "\n",
      "y : [ 3  5  7  9 11] , predict : [ 2.984122   4.9902043  6.996287   9.00237   11.008452 ]\n"
     ]
    }
   ],
   "source": [
    "# 19. 간단한 공식을 이용해서 값을 전달한 후 평가로 예측 값을 구현해보자\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# y = ax + b\n",
    "# 5 = a2 + b3 = a1 + b7 = a*3 + b\n",
    "\n",
    "# 1단계) 데이터\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = x*2 + 1\n",
    "print(x)\n",
    "print(y, '\\n')\n",
    "\n",
    "\n",
    "# 2단계) 학습 구조 모델링\n",
    "model = Sequential()  # 레이어를 층층이 쌓아주는 메소드\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.compile('SGD', 'mse')\n",
    "# SGD(Stochastic Gradient Descent) : 확률적 경사 하강법\n",
    "# mse(Mean Squered Error : 평균 제곱 오차)\n",
    "\n",
    "\n",
    "# 3단계) 학습 수행 : 몇 번을 어떻게 할꺼야?\n",
    "model.fit(x, y, epochs=1000, verbose=0)  \n",
    "# epochs : 샘플데이터로 몇 번을 수행할건지\n",
    "\n",
    "\n",
    "# 4단계) 평가\n",
    "print('y :', y, ', predict :', model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[8. 8.]\n",
      " [8. 8.]], shape=(2, 2), dtype=float32) \n",
      "\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 20. tf.GradientTape API : 텐서플로는 자동 미분하는 API를 제공한다\n",
    "# context 안에 실행된 모든 연산을 tape에 기록한다.\n",
    "# 후진방식 자동미분을 사용해서 tape에 기록된 연산 결과를 계산한다.\n",
    "# 기울기를 구하는 클래스 : 정밀한 예측 모델을 만들려면 적당히 선택한 매개 변수에서 \n",
    "# 예상한 값과 실제 결과를 비교해서 가능한 차이가 적게 매개변수를 조정하게 된다.\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y,y)\n",
    "    \n",
    "dz_dx = t.gradient(z,x)  # 입력텐서 x에 대한 z값\n",
    "print(dz_dx, '\\n')\n",
    "\n",
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "#         assert dz_dx[i][j] == 8.0\n",
    "        dz_dx[i][j].numpy() == 8.0\n",
    "        print(dz_dx[i][j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
