{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sister']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')  # 2) BeautifulSoup(문서대상, 'html.parser')  문서대상을 다운로드한다.\n",
    "\n",
    "# print(soup.prettify())\n",
    "soup.p['class']  # 속성\n",
    "soup.a['href']\n",
    "\n",
    "for res in soup.find_all('a'):\n",
    "    print(res.get('href'))\n",
    "# soup.find('a')\n",
    "\n",
    "soup.find(id='link3')['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "import requests\n",
    "\n",
    "html_res = requests.get('https://www.daum.net')  # URL 프로토콜로 요청\n",
    "html_res.content  # 문서내용\n",
    "\n",
    "soup = BeautifulSoup(html_res.content, 'html.parser')  # 2) BeautifulSoup(문서대상, 'html.parser')  문서대상을 다운로드한다.\n",
    "\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div class=\"ecopyramid\">\n",
      "   <ul id=\"producers\">\n",
      "    <li class=\"producerlist\">\n",
      "     <div class=\"name\">\n",
      "      plants\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      100000\n",
      "     </div>\n",
      "    </li>\n",
      "    <li class=\"producerlist\">\n",
      "     <div class=\"name\">\n",
      "      algae\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      100000\n",
      "     </div>\n",
      "    </li>\n",
      "   </ul>\n",
      "   <ul id=\"primaryconsumers\">\n",
      "    <li class=\"primaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      deer\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      1000\n",
      "     </div>\n",
      "    </li>\n",
      "    <li class=\"primaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      rabbit\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      2000\n",
      "     </div>\n",
      "    </li>\n",
      "   </ul>\n",
      "   <ul id=\"secondaryconsumers\">\n",
      "    <li class=\"secondaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      fox\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      100\n",
      "     </div>\n",
      "    </li>\n",
      "    <li class=\"secondaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      bear\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      100\n",
      "     </div>\n",
      "    </li>\n",
      "   </ul>\n",
      "   <ul id=\"tertiaryconsumers\">\n",
      "    <li class=\"tertiaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      lion\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      80\n",
      "     </div>\n",
      "    </li>\n",
      "    <li class=\"tertiaryconsumerlist\">\n",
      "     <div class=\"name\">\n",
      "      tiger\n",
      "     </div>\n",
      "     <div class=\"number\">\n",
      "      50\n",
      "     </div>\n",
      "    </li>\n",
      "   </ul>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# 1. test02.html의 문서를 파싱하고 파싱된 내용을 출력하자\n",
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "import requests\n",
    "\n",
    "f = open(\"test02.html\")  # 2) 문서로 읽어온 후\n",
    "soup = BeautifulSoup(f, 'html.parser')  # 3) 파싱한다\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"name\">plants</div>, <div class=\"name\">algae</div>, <div class=\"name\">deer</div>, <div class=\"name\">rabbit</div>, <div class=\"name\">fox</div>, <div class=\"name\">bear</div>, <div class=\"name\">lion</div>, <div class=\"name\">tiger</div>]\n"
     ]
    }
   ],
   "source": [
    "# 2. test02 class이름 name에 접근해서 데이터를 리턴받자\n",
    "result = soup.find_all(class_=\"name\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"name\">plants</div> plants\n",
      "<div class=\"name\">algae</div> algae\n",
      "<div class=\"name\">deer</div> deer\n",
      "<div class=\"name\">rabbit</div> rabbit\n",
      "<div class=\"name\">fox</div> fox\n",
      "<div class=\"name\">bear</div> bear\n",
      "<div class=\"name\">lion</div> lion\n",
      "<div class=\"name\">tiger</div> tiger\n"
     ]
    }
   ],
   "source": [
    "# 3. 2번에서 text만 리턴받자 get_text()로 값을 리턴\n",
    "for i in result:\n",
    "    print(i, i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "1000\n",
      "2000\n",
      "100\n",
      "100\n",
      "80\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 4. test02에서 class 이름중에 number만 추출해서 값만 리턴받자\n",
    "res = soup.find_all(class_=\"number\")\n",
    "for i in res:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 80, 100, 100, 1000, 2000, 100000, 100000]\n"
     ]
    }
   ],
   "source": [
    "# 5. test02에서 class 이름중에 number만 추출해서 값만 리턴받자\n",
    "# []  m_list 객체에 값을 담아서 출력하고 정렬해보자\n",
    "\n",
    "m_list = []  # 객체 변수를 선언\n",
    "for i in res:\n",
    "    m_list.append(int(i.get_text()))\n",
    "    \n",
    "m_list.sort()\n",
    "print(m_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 01\n",
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "import urllib.request  # 웹 상의 url을 파이썬이 인식시켜 주는 모듈\n",
    "\n",
    "list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106'\n",
    "url = urllib.request.Request(list_url)  # Request : str -> http 프로토콜의 url 생성해주는 클래스\n",
    "\n",
    "# url 연결상태에서 해당 요청 페이지를 읽어서 리턴\n",
    "result = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 02 : 위에 전체 내용을 읽어 온 것을 html로 파싱 할 수 있도록 BeautifulSoup을 사용하자\n",
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "import urllib.request  # 웹 상의 url을 파이썬이 인식시켜 주는 모듈\n",
    "\n",
    "list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106'\n",
    "url = urllib.request.Request(list_url)  # Request : str -> http 프로토콜의 url 생성해주는 클래스\n",
    "\n",
    "# url 연결상태에서 해당 요청 페이지를 읽어서 리턴\n",
    "result = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(result, 'html.parser')   # 전체 문서 str -> html 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 03 : \"최고의 히어로 만화에요.\" 가 해당되는 태그와 클래스를 찾아보자\n",
    "<p class=\"con\">\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t    \n",
    "\t\t\t\t\t\t\t  \t    최고의 히어로 만화에요.\n",
    "\t\t\t\t\t\t\t    \n",
    "\t\t\t\t\t\t    \n",
    "\t\t\t\t\t\t    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# exam 04 : p 태그 중에 class가 con에 해당하는 부분을 크롤링 하자\n",
    "# 단 글자들만 즉 값만 추출하자\n",
    "\n",
    "p_res = soup.find_all('p', class_ = 'con')\n",
    "\n",
    "for i in p_res:\n",
    "    print(i.get_text(\" \",strip=True))  # strip : 앞뒤로 공백제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exam 05 : exam 04에 있는 내용을 my_res [] 리스트에 담자\n",
    "\n",
    "p_res = soup.find_all('p', class_ = 'con')\n",
    "\n",
    "my_res = []\n",
    "for i in p_res:\n",
    "    my_res.append(i.get_text(\" \",strip=True))\n",
    "    \n",
    "my_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 06 : 시청자 게시판의 날짜와 본문을 가져오자\n",
    "# <span class=\"date\">2020.12.18 22:33</span>\n",
    "\n",
    "p_res = soup.find_all('p', class_ = 'con')  # 본문\n",
    "p_date = soup.find_all('span', class_ = 'date')  # 날짜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 07 : param01, param02 리스트에 각각 담자\n",
    "\n",
    "param01 = []\n",
    "param02 = []\n",
    "for i in p_res:\n",
    "    param01.append(i.get_text(\" \",strip=True))\n",
    "    \n",
    "for i in p_date:\n",
    "    param02.append(i.get_text(\" \",strip=True))\n",
    "    \n",
    "print(param01)\n",
    "print(param02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 08 : 2020.12.18 22:33 최고의 히어로 만화에요.\n",
    "\n",
    "for d,c in zip(param02, param01):\n",
    "    print(d + '\\t' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이디 버그 전체 페이지의 url을 탐색해보자\n",
    "for i in range(1, 23):\n",
    "    print('http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+str(i)+\n",
    "        '&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam 09 : 레이디버그의 시청자 게시판에 있는 모든 글을 크롤링 해서 날짜 내용을 입력 한 후 파일에 저장해보자\n",
    "from bs4 import BeautifulSoup  # 1) 모듈을 임포트 한다\n",
    "import urllib.request  # 웹 상의 url을 파이썬이 인식시켜 주는 모듈\n",
    "\n",
    "# 1) 전역 변수 선언\n",
    "param01 = []\n",
    "param02 = []\n",
    "\n",
    "# 2) 저장할 파일 선언\n",
    "f = open('mytext01.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# url 읽어오고\n",
    "for i in range(1, 23):\n",
    "    list_url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page='+str(i)+'&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'\n",
    "    \n",
    "# 파싱 작업\n",
    "    url = urllib.request.Request(list_url)\n",
    "\n",
    "# url 연결상태에서 해당 요청 페이지를 읽어서 리턴\n",
    "    result = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(result, 'html.parser')   # 전체 문서 str -> html 파싱\n",
    "    \n",
    "# 시청자 게시판 날짜 및 본문의 내용에 관한 것을 가져온다\n",
    "    p_res = soup.find_all('p', class_ = 'con')  # 본문\n",
    "    p_date = soup.find_all('span', class_ = 'date')  # 날짜\n",
    "    \n",
    "# 리스트 객체에 담는다\n",
    "    for i in p_res:\n",
    "        param01.append(i.get_text(\" \",strip=True))\n",
    "    \n",
    "    for i in p_date:\n",
    "        param02.append(i.get_text(\" \",strip=True))\n",
    "\n",
    "# 날짜와 본문을 함께 파일에 저장한다.\n",
    "for d,c in zip(param02, param01):\n",
    "    f.write(d + ' ' + c + '\\n')\n",
    "        \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
